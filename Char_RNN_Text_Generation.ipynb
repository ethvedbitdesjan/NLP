{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Swamy_Vivekanand_Reproduce.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hczP4pa4AWdr",
        "outputId": "3387731c-30e7-48d7-ed4f-ba30a3499bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% cd '/content/gdrive/MyDrive/Char_RNN'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rq3qoJWAZHP",
        "outputId": "db544dc4-d522-4eea-cff1-c31b4f111293"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Char_RNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('text.txt', 'r') as f:\n",
        "    text = [line for line in f]\n",
        "\n",
        "print(text[10:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrTcF4veAcii",
        "outputId": "4b55bff2-e8fe-4183-ba6f-ede76c73b2ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the delegates from the Orient, have told you that these men from far-off \\n', 'nations may well claim the honour of bearing to different lands the idea of \\n', 'toleration. I am proud to belong to a religion which has taught the world both \\n', 'tolerance and universal acceptance. We believe not only in universal toleration, \\n', 'but we accept all religions as true. I am proud to belong to a nation which has \\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[30:32]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT6S-16hAogJ",
        "outputId": "9e743868-49a6-4905-d167-8400b6162f4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doctrine preached in the Gita: â€œWhosoever comes to Me, through whatsoever \\n',\n",
              " 'form, I reach him; all men are struggling through paths which in the end lead to \\n']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev = -1 #index of last group of '\\n'\n",
        "prev_word_idx = 1 #index of the last item that was not '\\n' \n",
        "prev_n = -1 \n",
        "continuous= 0 #counting the consecutive '\\n'\n",
        "list_idx_to_remove = []\n",
        "for i in range(2, len(text)):\n",
        "  if text[i]=='\\n':\n",
        "    continuous+= 1\n",
        "  else:\n",
        "    prev_word_idx = i\n",
        "    continuous = 0\n",
        "  if continuous>1:\n",
        "    if (prev> -1) and (i - prev)==5:\n",
        "      if len(text[i-3])<40 and text[i-3]!='\\n':\n",
        "        list_idx_to_remove.append(i-3)\n",
        "        #remove next sentence if only the first is removed\n",
        "        if len(text[i-2])<40 and text[i-2]!='\\n':\n",
        "          list_idx_to_remove.append(i-2)\n",
        "    if(prev>-1) and (i-prev)==4:\n",
        "      if len(text[i-2])<40 and text[i-2]!='\\n':\n",
        "        list_idx_to_remove.append(i-2)\n",
        "    if continuous>2:\n",
        "      list_idx_to_remove.append(prev_word_idx)\n",
        "    prev = i-continuous + 1\n",
        "\n",
        "print(len(list_idx_to_remove), list_idx_to_remove[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTJ5qXlMArB6",
        "outputId": "663587de-98b2-46fb-9ab7-82374bc66509"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2782 [45, 46, 92, 540, 541]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_idx_to_remove = list(set(list_idx_to_remove))\n",
        "list_idx_to_remove.sort()\n",
        "print(len(list_idx_to_remove))\n",
        "for idx_to_remove in list_idx_to_remove[11:13]:\n",
        "  print(text[idx_to_remove -3:idx_to_remove+3])\n",
        "  print(text[idx_to_remove])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sXxNIkfJ8ee",
        "outputId": "79d658cc-434a-4745-ddd3-c731062d38c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2362\n",
            "['renunciation. \\n', '\\n', '\\n', 'Chapter IV. What Is Duty? \\n', '\\n', '\\n']\n",
            "Chapter IV. What Is Duty? \n",
            "\n",
            "['from each other, they at last lead to the same goal of human perfection. \\n', '\\n', '\\n', 'Chapter VII. Freedom \\n', '\\n', '\\n']\n",
            "Chapter VII. Freedom \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = []\n",
        "continuous = 0 #the number of consecutive '\\n'\n",
        "\n",
        "for i in range(len(text)):\n",
        "  line = text[i]\n",
        "  if line=='\\n':\n",
        "    continuous +=1\n",
        "  else:\n",
        "    continuous = 0\n",
        "  \n",
        "  if i in list_idx_to_remove:\n",
        "    continue\n",
        "  \n",
        "  if continuous>3:\n",
        "    continue\n",
        "  cleaned_text.append(line)\n",
        "\n",
        "print(len(cleaned_text), len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PJNLdelME9g",
        "outputId": "8c833735-0f78-464d-bb15-c6303a7f5693"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "146561 149291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = \"\".join(cleaned_text)"
      ],
      "metadata": {
        "id": "aIpLuABSMosP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchtext\n",
        "from collections import defaultdict\n",
        "import random"
      ],
      "metadata": {
        "id": "Cvhy-SUtSniX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = cleaned_text.lower()"
      ],
      "metadata": {
        "id": "4xj7jmr7vSSd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(cleaned_text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in cleaned_text])\n",
        "encoded[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6edsDBqOHvW",
        "outputId": "4e6d7c6f-78a9-4cfc-c756-dd08ac2cc094"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([53, 40, 32, 40,  0, 28, 34, 40, 25, 55, 81, 61, 25, 48, 34, 10,  0,\n",
              "       27, 28, 34, 40, 25, 10, 50, 25, 55, 78, 28, 34, 32, 74, 55, 62, 25,\n",
              "       53, 53, 32,  0, 25, 50, 32,  3,  3, 40, 25, 78, 71, 25, 27, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "  return one_hot"
      ],
      "metadata": {
        "id": "BrHFjJK_S4Iu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = np.array([[2, 6, 5]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "print(one_hot, one_hot.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rllCzjLmTbFY",
        "outputId": "7b5c3e3b-a85f-4df4-921d-e551d54f6fad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]]] (1, 3, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''returns batches of size batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded in a sequence\n",
        "    '''\n",
        "\n",
        "    total_batch_size = batch_size*seq_length\n",
        "    num_batches = len(arr)//total_batch_size\n",
        "\n",
        "    batches = []\n",
        "    for i in range(0, len(arr), total_batch_size):\n",
        "      if i+total_batch_size>len(arr):\n",
        "        break\n",
        "      chars_of_batch = arr[i:i+total_batch_size]\n",
        "\n",
        "      x = []\n",
        "      #x = [chars_of_batch[j*seq_length:j*seq_length] for j in range(batch_size)]\n",
        "      for j in range(batch_size):\n",
        "        one_seq = chars_of_batch[j*seq_length:(j+1)*seq_length]\n",
        "        x.append(one_seq)\n",
        "      x = np.array(x)\n",
        "\n",
        "      #print(x, x.shape)\n",
        "      y = np.zeros_like(x)\n",
        "      try:\n",
        "        y[:-1, -1] = x[1:, 0]\n",
        "      except:\n",
        "        print(x.shape, y.shape, i, len(arr), total_batch_size)\n",
        "        y[:-1, -1] = x[1:, 0]\n",
        "      try:\n",
        "        y[:, :-1], y[-1, -1] = x[:, 1:], arr[i*seq_length]\n",
        "      except IndexError:\n",
        "        y[:, :-1], y[-1, -1] = x[:, 1:], arr[i+total_batch_size-1]\n",
        "      \n",
        "      \n",
        "      #print(y, y.shape)\n",
        "\n",
        "      yield x, y\n",
        "      \n",
        "      \n",
        "        \n"
      ],
      "metadata": {
        "id": "hrslzC3sTg0M"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "Vlo9-pCxYmfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a57b11-f326-4fc7-aefb-59a67caaf4ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 50) (8, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0lvDw8qbXj6",
        "outputId": "98191cde-e316-4728-d25e-4925f9947fb5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "7gRuE5iQbe1d"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, dropout=0.2, lr=0.0001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = dropout\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=dropout, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "        \n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        \n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # out.shape = [batch_size*seq_length, len(self.chars)]\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "metadata": {
        "id": "c0i9KDmUYn5P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    prime = prime.lower()\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "tt4NiSjr1QJf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        #print(inputs.shape, \"input.shape\")\n",
        "        #print(h[0].shape, h[1].shape, \"hidd.shape\")\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "metadata": {
        "id": "QNIBIriE1YUC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.2, print_every=300):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    global min_val_loss\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            #print(inputs.shape, \"inputs\")\n",
        "            output, h = net(inputs, h)\n",
        "            #print(output.shape, \"outputs\")\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            \n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                sample_written = sample(net, 200, prime='The people', top_k=3)\n",
        "                try:\n",
        "                  with open('Example2.txt', 'w') as writefile:\n",
        "                    writefile.write(sample_written+\"\\n\")\n",
        "                except:\n",
        "                  print(sample_written[:75])\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    \n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                if np.mean(val_losses)<min_val_loss and round(random.random())==1:\n",
        "                  min_val_loss = np.mean(val_losses)\n",
        "                  model_name = 'rnn_22_epoch.net'\n",
        "                  checkpoint = {'n_hidden': net.n_hidden,\n",
        "                                'n_layers': net.n_layers,\n",
        "                                'state_dict': net.state_dict(),\n",
        "                                'tokens': net.chars}\n",
        "\n",
        "                  with open(model_name, 'wb') as f:\n",
        "                    torch.save(checkpoint, f)\n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "metadata": {
        "id": "O3p8nGMXcQK_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden=512\n",
        "n_layers=3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "total_params = sum(\n",
        "\tparam.numel() for param in net.parameters()\n",
        ")\n",
        "print(total_params)"
      ],
      "metadata": {
        "id": "SyWEwfVDc9J2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a30749a-d6ac-44af-f209-5e2afe1f45d9"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(82, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=82, bias=True)\n",
            ")\n",
            "5465170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "jmbNKBtg3leF"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ExponentialLR "
      ],
      "metadata": {
        "id": "SPKHWAOPEK0i"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "seq_length = 300\n",
        "n_epochs = 25 # start small if testing initial behavior\n",
        "# train the model\n",
        "min_val_loss = 1e7\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=300)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyZDlcP4dUgE",
        "outputId": "67899246-7ce7-493f-a20e-36ce47738115"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/25... Step: 300... Loss: 2.4945... Val Loss: 2.4410\n",
            "Epoch: 2/25... Step: 600... Loss: 1.8750... Val Loss: 1.8997\n",
            "Epoch: 3/25... Step: 900... Loss: 1.6730... Val Loss: 1.6434\n",
            "Epoch: 4/25... Step: 1200... Loss: 1.4784... Val Loss: 1.5463\n",
            "Epoch: 5/25... Step: 1500... Loss: 1.3799... Val Loss: 1.4735\n",
            "Epoch: 6/25... Step: 1800... Loss: 1.2502... Val Loss: 1.4329\n",
            "Epoch: 7/25... Step: 2100... Loss: 1.2821... Val Loss: 1.3875\n",
            "Epoch: 8/25... Step: 2400... Loss: 1.2669... Val Loss: 1.3341\n",
            "Epoch: 8/25... Step: 2700... Loss: 1.2651... Val Loss: 1.3220\n",
            "Epoch: 9/25... Step: 3000... Loss: 1.4307... Val Loss: 1.3060\n",
            "Epoch: 10/25... Step: 3300... Loss: 1.1999... Val Loss: 1.2801\n",
            "Epoch: 11/25... Step: 3600... Loss: 1.2876... Val Loss: 1.2830\n",
            "Epoch: 12/25... Step: 3900... Loss: 1.1425... Val Loss: 1.2764\n",
            "Epoch: 13/25... Step: 4200... Loss: 1.2640... Val Loss: 1.2815\n",
            "Epoch: 14/25... Step: 4500... Loss: 1.0760... Val Loss: 1.2610\n",
            "Epoch: 15/25... Step: 4800... Loss: 1.0744... Val Loss: 1.2435\n",
            "Epoch: 15/25... Step: 5100... Loss: 1.2236... Val Loss: 1.2365\n",
            "Epoch: 16/25... Step: 5400... Loss: 1.2966... Val Loss: 1.2253\n",
            "Epoch: 17/25... Step: 5700... Loss: 1.1815... Val Loss: 1.2258\n",
            "Epoch: 18/25... Step: 6000... Loss: 1.1576... Val Loss: 1.2274\n",
            "Epoch: 19/25... Step: 6300... Loss: 1.0889... Val Loss: 1.2296\n",
            "Epoch: 20/25... Step: 6600... Loss: 1.1003... Val Loss: 1.2267\n",
            "Epoch: 21/25... Step: 6900... Loss: 1.1044... Val Loss: 1.2280\n",
            "Epoch: 22/25... Step: 7200... Loss: 1.0443... Val Loss: 1.2212\n",
            "Epoch: 22/25... Step: 7500... Loss: 1.1320... Val Loss: 1.2119\n",
            "Epoch: 23/25... Step: 7800... Loss: 1.1642... Val Loss: 1.1969\n",
            "Epoch: 24/25... Step: 8100... Loss: 1.1373... Val Loss: 1.2048\n",
            "Epoch: 25/25... Step: 8400... Loss: 1.1045... Val Loss: 1.1997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "metadata": {
        "id": "9RY7FkqaqPiC"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 150, prime='The world', top_k=3))"
      ],
      "metadata": {
        "id": "6svL9jUCqprO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1855be60-e1cc-4802-d9fd-62c920513c8f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the worldly spirituality is to be \n",
            "able. the soul which is not to be coloured, as the same way as it were and \n",
            "they will be superior to the senses as it is. if \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7-knR0LvmUmO"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden=256\n",
        "n_layers=6\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "total_params = sum(\n",
        "\tparam.numel() for param in net.parameters()\n",
        ")\n",
        "print(total_params)"
      ],
      "metadata": {
        "id": "5LDJkjz30Eeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d345fe9-9cbf-4803-b9f2-c3be08fb4a28"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(82, 256, num_layers=6, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=82, bias=True)\n",
            ")\n",
            "3000914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 250\n",
        "n_epochs = 30 # start smaller if you are just testing initial behavior\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.002, print_every=300)"
      ],
      "metadata": {
        "id": "IMPJgd3T0K25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371632f9-869d-4a97-8ec2-6e8886160841"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/30... Step: 300... Loss: 2.9625... Val Loss: 3.0162\n",
            "Epoch: 3/30... Step: 600... Loss: 2.9813... Val Loss: 3.0118\n",
            "Epoch: 5/30... Step: 900... Loss: 2.9417... Val Loss: 3.0208\n",
            "Epoch: 6/30... Step: 1200... Loss: 3.0088... Val Loss: 3.0064\n",
            "Epoch: 8/30... Step: 1500... Loss: 2.9514... Val Loss: 3.0132\n",
            "Epoch: 9/30... Step: 1800... Loss: 3.0102... Val Loss: 3.0092\n",
            "Epoch: 11/30... Step: 2100... Loss: 2.9299... Val Loss: 3.0158\n",
            "Epoch: 12/30... Step: 2400... Loss: 2.9552... Val Loss: 3.0090\n",
            "Epoch: 14/30... Step: 2700... Loss: 2.9294... Val Loss: 3.0095\n",
            "Epoch: 15/30... Step: 3000... Loss: 2.9756... Val Loss: 3.0089\n",
            "Epoch: 17/30... Step: 3300... Loss: 2.9372... Val Loss: 3.0132\n",
            "Epoch: 18/30... Step: 3600... Loss: 2.9597... Val Loss: 3.0135\n",
            "Epoch: 20/30... Step: 3900... Loss: 2.9461... Val Loss: 3.0080\n",
            "Epoch: 21/30... Step: 4200... Loss: 2.9392... Val Loss: 3.0139\n",
            "Epoch: 22/30... Step: 4500... Loss: 2.9700... Val Loss: 3.0087\n",
            "Epoch: 24/30... Step: 4800... Loss: 2.9410... Val Loss: 3.0177\n",
            "Epoch: 25/30... Step: 5100... Loss: 2.9993... Val Loss: 3.0064\n",
            "Epoch: 27/30... Step: 5400... Loss: 2.9183... Val Loss: 3.0132\n",
            "Epoch: 28/30... Step: 5700... Loss: 2.9942... Val Loss: 3.4892\n",
            "Epoch: 30/30... Step: 6000... Loss: 2.9277... Val Loss: 3.0156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden=1024\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "total_params = sum(\n",
        "\tparam.numel() for param in net.parameters()\n",
        ")\n",
        "print(total_params)"
      ],
      "metadata": {
        "id": "OrYvhUbk0PB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a8782b-d4a7-412b-c41f-6135f834ad8c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(82, 1024, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=82, bias=True)\n",
            ")\n",
            "13019218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "w__cy6520rvL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 400\n",
        "n_epochs = 45 # start smaller if just testing initial behavior\n",
        "# train the model\n",
        "min_val_loss = 1.2435\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.0005, print_every=300)"
      ],
      "metadata": {
        "id": "AWNly5bIX-Xe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e7c6a4-5b7f-4f0f-8fd3-32dddab5066c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/45... Step: 300... Loss: 1.4345... Val Loss: 1.5807\n",
            "Epoch: 5/45... Step: 600... Loss: 1.3657... Val Loss: 1.4212\n",
            "Epoch: 8/45... Step: 900... Loss: 1.2018... Val Loss: 1.3517\n",
            "Epoch: 10/45... Step: 1200... Loss: 1.1286... Val Loss: 1.3066\n",
            "Epoch: 12/45... Step: 1500... Loss: 1.2987... Val Loss: 1.2708\n",
            "Epoch: 15/45... Step: 1800... Loss: 1.1121... Val Loss: 1.2590\n",
            "Epoch: 17/45... Step: 2100... Loss: 1.0556... Val Loss: 1.2429\n",
            "Epoch: 19/45... Step: 2400... Loss: 1.1024... Val Loss: 1.2213\n",
            "Epoch: 22/45... Step: 2700... Loss: 1.0706... Val Loss: 1.2261\n",
            "Epoch: 24/45... Step: 3000... Loss: 1.0960... Val Loss: 1.2180\n",
            "Epoch: 26/45... Step: 3300... Loss: 1.1464... Val Loss: 1.2043\n",
            "Epoch: 29/45... Step: 3600... Loss: 1.0042... Val Loss: 1.2271\n",
            "Epoch: 31/45... Step: 3900... Loss: 1.0651... Val Loss: 1.2174\n",
            "Epoch: 33/45... Step: 4200... Loss: 1.0310... Val Loss: 1.2088\n",
            "Epoch: 36/45... Step: 4500... Loss: 0.9233... Val Loss: 1.2389\n",
            "Epoch: 38/45... Step: 4800... Loss: 0.8975... Val Loss: 1.2369\n",
            "Epoch: 40/45... Step: 5100... Loss: 0.9405... Val Loss: 1.2279\n",
            "Epoch: 43/45... Step: 5400... Loss: 0.8868... Val Loss: 1.2558\n",
            "Epoch: 45/45... Step: 5700... Loss: 0.9768... Val Loss: 1.2625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 150, prime='India is a', top_k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiPLv-JjTdtO",
        "outputId": "932749f4-4dc8-4fe0-cb65-2400ee600f88"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "india is a chance of \n",
            "practice. the superconscious state of ten times, in all the bad are the \n",
            "prophets of the country and the palace of the wave and the protect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = [line for line in f]\n",
        "\n",
        "print(text[10:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbiReG4ya0gw",
        "outputId": "21fe8018-a617-47ec-da8c-61a5a7fa77c1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Resolved. resolved.\\n', '\\n', 'First Citizen:\\n', 'First, you know Caius Marcius is chief enemy to the people.\\n', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = \"\".join(text)"
      ],
      "metadata": {
        "id": "5QLAkp6Ra7Vr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaned_text = cleaned_text.lower()\n",
        "chars = list(set(cleaned_text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in cleaned_text])\n",
        "encoded[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lstvNk9pbJjy",
        "outputId": "3c43737c-43c4-447a-fbbf-22ecb3d960db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([48, 26, 33, 59,  1,  2, 13, 26,  1, 26, 50, 19, 64,  5, 60, 35, 19,\n",
              "       55, 24, 33, 19,  2, 17, 19,  2, 20, 33, 24, 49, 19, 19,  9,  2, 63,\n",
              "       64, 34,  2, 55, 52, 33,  1, 15, 19, 33, 10,  2, 15, 19, 63, 33])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden=512\n",
        "n_layers=3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "total_params = sum(\n",
        "\tparam.numel() for param in net.parameters()\n",
        ")\n",
        "print(total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNffxjJHbd0y",
        "outputId": "1629d0b3-3fee-44ac-85d3-14724dca2de6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n",
            "5421633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "seq_length = 300\n",
        "n_epochs = 50 # start small if testing initial behavior\n",
        "# train the model\n",
        "min_val_loss = 1e7\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDOAjx_9b0Ic",
        "outputId": "d6b91984-cb39-42ed-de8a-1f339c38806f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/50... Step: 100... Loss: 3.1664... Val Loss: 3.1697\n",
            "Epoch: 5/50... Step: 200... Loss: 2.7007... Val Loss: 2.6480\n",
            "Epoch: 7/50... Step: 300... Loss: 2.2959... Val Loss: 2.2797\n",
            "Epoch: 9/50... Step: 400... Loss: 2.0667... Val Loss: 2.0839\n",
            "Epoch: 11/50... Step: 500... Loss: 1.9512... Val Loss: 1.9445\n",
            "Epoch: 14/50... Step: 600... Loss: 1.8157... Val Loss: 1.8405\n",
            "Epoch: 16/50... Step: 700... Loss: 1.6664... Val Loss: 1.7688\n",
            "Epoch: 18/50... Step: 800... Loss: 1.6103... Val Loss: 1.7302\n",
            "Epoch: 20/50... Step: 900... Loss: 1.6141... Val Loss: 1.6811\n",
            "Epoch: 22/50... Step: 1000... Loss: 1.4051... Val Loss: 1.6434\n",
            "Epoch: 24/50... Step: 1100... Loss: 1.5521... Val Loss: 1.6124\n",
            "Epoch: 27/50... Step: 1200... Loss: 1.4112... Val Loss: 1.5905\n",
            "Epoch: 29/50... Step: 1300... Loss: 1.3120... Val Loss: 1.5754\n",
            "Epoch: 31/50... Step: 1400... Loss: 1.3400... Val Loss: 1.5751\n",
            "Epoch: 33/50... Step: 1500... Loss: 1.3570... Val Loss: 1.5705\n",
            "Epoch: 35/50... Step: 1600... Loss: 1.2466... Val Loss: 1.5678\n",
            "Epoch: 37/50... Step: 1700... Loss: 1.3334... Val Loss: 1.5510\n",
            "Epoch: 40/50... Step: 1800... Loss: 1.2638... Val Loss: 1.5414\n",
            "Epoch: 42/50... Step: 1900... Loss: 1.2280... Val Loss: 1.5448\n",
            "Epoch: 44/50... Step: 2000... Loss: 1.2234... Val Loss: 1.5511\n",
            "Epoch: 46/50... Step: 2100... Loss: 1.2605... Val Loss: 1.5591\n",
            "Epoch: 48/50... Step: 2200... Loss: 1.3308... Val Loss: 1.5724\n",
            "Epoch: 50/50... Step: 2300... Loss: 1.2130... Val Loss: 1.5726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 150, prime='she is the east', top_k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSIVuUuhhQdk",
        "outputId": "dbf32faa-87dd-4291-a7af-a8ba5dc76350"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "she is the east,\n",
            "And with the sea, that will not stay to hear\n",
            "Where they did be the father that they say a man\n",
            "We have seen the subjects of the caunter's son.\n",
            "\n",
            "RICHAR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 550, prime=\"\"\"A thing slipp'd idly from me.\n",
        "Our poesy is as a gum, which oozes\n",
        "From whence 'tis nourish'd: the fire i' the flint\n",
        "Shows not till it be struck; our gentle flame\"\"\", top_k=3)) #The above string is from an unpublished shapespeare play so is new to the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k_WFu-viFOw",
        "outputId": "123f0fa8-b792-4157-d3d8-4c685951a268"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a thing slipp'd idly from me.\n",
            "our poesy is as a gum, which oozes\n",
            "from whence 'tis nourish'd: the fire i' the flint\n",
            "shows not till it be struck; our gentle flame\n",
            "As the most sweet way is a man of that woe\n",
            "To me to the courage. I am so defence where I stand for the compouse,\n",
            "And therefore have the world was most abroad;\n",
            "For what is my brother's face, and they she'll see\n",
            "What you are the succession of the world with thee\n",
            "The senaties of his father, to the sea,\n",
            "And there and there the straight of me to straight\n",
            "The palace that we with the chambing and\n",
            "the shepherd's, an all this is the common thousand things\n",
            "As that I seek to speak. Yet! what, we have some common\n",
            "Who is that words that thou hast shall not \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lines that follow the above words are:\n",
        "\n",
        "*Shows not till it be struck; our gentle flame\n",
        "Provokes itself and like the current flies\n",
        "Each bound it chafes. What have you there?*\n",
        "\n",
        "Our model accurately predicts a complete line. This model has not been trained for long and though it has a significant cross entropy loss, it still writes english pretty correctly especially considering the amount of punctuation Shakespeare used and the incomplete words too!"
      ],
      "metadata": {
        "id": "A72PAX77jaM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ud2G9SIjY5Gs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}